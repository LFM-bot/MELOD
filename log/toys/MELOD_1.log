2023-01-06 12:15:18 INFO     log save at : log\toys\MELOD_1.log
2023-01-06 12:15:18 INFO     model save at: save\MELOD-toys-2023-01-06_12-15-18.pth
2023-01-06 12:15:18 INFO     [1] Model Hyper-Parameter ---------------------
2023-01-06 12:15:18 INFO     model: MELOD
2023-01-06 12:15:18 INFO     embed_size: 128
2023-01-06 12:15:18 INFO     DIIN_loss_type: MR
2023-01-06 12:15:18 INFO     alpha: 0.1
2023-01-06 12:15:18 INFO     lamda: 0.5
2023-01-06 12:15:18 INFO     sas_prob: 3
2023-01-06 12:15:18 INFO     freeze_kg: False
2023-01-06 12:15:18 INFO     episode_num: 1
2023-01-06 12:15:18 INFO     episode_len: 3
2023-01-06 12:15:18 INFO     hit_range: 100
2023-01-06 12:15:18 INFO     hit_r: 1.0
2023-01-06 12:15:18 INFO     sample_size: 100
2023-01-06 12:15:18 INFO     prob_sharpen: 1.0
2023-01-06 12:15:18 INFO     num_blocks: 2
2023-01-06 12:15:18 INFO     num_heads: 2
2023-01-06 12:15:18 INFO     ffn_hidden: 256
2023-01-06 12:15:18 INFO     attn_dropout: 0.5
2023-01-06 12:15:18 INFO     ffn_dropout: 0.5
2023-01-06 12:15:18 INFO     model_type: Knowledge
2023-01-06 12:15:18 INFO     loss_type: CUSTOM
2023-01-06 12:15:18 INFO     [2] Experiment Hyper-Parameter ----------------
2023-01-06 12:15:18 INFO     [2-1] data hyper-parameter --------------------
2023-01-06 12:15:18 INFO     dataset: toys
2023-01-06 12:15:18 INFO     data_aug: True
2023-01-06 12:15:18 INFO     seq_filter_len: 3
2023-01-06 12:15:18 INFO     if_filter_target: True
2023-01-06 12:15:18 INFO     use_tar_len: True
2023-01-06 12:15:18 INFO     target_len: 3
2023-01-06 12:15:18 INFO     max_len: 50
2023-01-06 12:15:18 INFO     [2-2] pretraining hyper-parameter -------------
2023-01-06 12:15:18 INFO     do_pretraining: False
2023-01-06 12:15:18 INFO     pretraining_task: MISP
2023-01-06 12:15:18 INFO     pretraining_epoch: 10
2023-01-06 12:15:18 INFO     pretraining_batch: 512
2023-01-06 12:15:18 INFO     pretraining_lr: 0.001
2023-01-06 12:15:18 INFO     pretraining_l2: 0.0
2023-01-06 12:15:18 INFO     [2-3] training hyper-parameter ----------------
2023-01-06 12:15:18 INFO     epoch_num: 100
2023-01-06 12:15:18 INFO     train_batch: 512
2023-01-06 12:15:18 INFO     learning_rate: 0.001
2023-01-06 12:15:18 INFO     l2: 1e-06
2023-01-06 12:15:18 INFO     patience: 5
2023-01-06 12:15:18 INFO     device: cuda:0
2023-01-06 12:15:18 INFO     num_worker: 0
2023-01-06 12:15:18 INFO     [2-4] evaluation hyper-parameter --------------
2023-01-06 12:15:18 INFO     split_type: valid_and_test
2023-01-06 12:15:18 INFO     split_mode: LS_R@0.2
2023-01-06 12:15:18 INFO     eval_mode: uni100
2023-01-06 12:15:18 INFO     metric: ['hit', 'ndcg']
2023-01-06 12:15:18 INFO     k: [5, 10]
2023-01-06 12:15:18 INFO     valid_metric: hit@10
2023-01-06 12:15:18 INFO     eval_batch: 512
2023-01-06 12:15:18 INFO     [2-5] save hyper-parameter --------------------
2023-01-06 12:15:18 INFO     log_save: log
2023-01-06 12:15:18 INFO     model_save: save
2023-01-06 12:15:18 INFO     [3] Data Statistic ----------------------------
2023-01-06 12:15:18 INFO     dataset: toys
2023-01-06 12:15:18 INFO     user number: 19412
2023-01-06 12:15:18 INFO     item number: 11925
2023-01-06 12:15:18 INFO     average seq length: 8.6337
2023-01-06 12:15:18 INFO     density: 0.0007 sparsity: 0.9993
2023-01-06 12:15:18 INFO     data after augmentation:
2023-01-06 12:15:18 INFO     train samples: 128773	eval samples: 3882	test samples: 15530
2023-01-06 12:15:18 INFO     [1] Model Architecture ------------------------
2023-01-06 12:15:18 INFO     total parameters: 1864194
2023-01-06 12:15:18 INFO     MELOD(
  (indu_loss_func): MarginRankingLoss()
  (item_embedding): Embedding(11926, 128)
  (seq_encoder): Transformer(
    (pos_emb): Embedding(50, 128)
    (emb_dropout): Dropout(p=0.5, inplace=False)
    (attention_layernorms): ModuleList(
      (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
    )
    (attention_layers): ModuleList(
      (0): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (1): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
    (forward_layernorms): ModuleList(
      (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
    )
    (forward_layers): ModuleList(
      (0): PointWiseFeedForward(
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (act): ReLU()
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (dropout2): Dropout(p=0.5, inplace=False)
      )
      (1): PointWiseFeedForward(
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (act): ReLU()
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (dropout2): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (W1): Linear(in_features=256, out_features=256, bias=True)
  (W2): Linear(in_features=256, out_features=2, bias=True)
  (emb_drop): Dropout(p=0.5, inplace=False)
  (cos): CosineSimilarity()
)
2023-01-06 12:15:18 INFO     Start training...
2023-01-06 12:15:45 INFO     ----------------------------Epoch 1----------------------------
2023-01-06 12:15:45 INFO      Training Time :[27.3 s]	Training Loss = 4.5788
2023-01-06 12:15:45 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:15:45 INFO     hit@5:0.1796	hit@10:0.2111	ndcg@5:0.1167	ndcg@10:0.1272	
2023-01-06 12:16:11 INFO     ----------------------------Epoch 2----------------------------
2023-01-06 12:16:11 INFO      Training Time :[26.0 s]	Training Loss = 4.2297
2023-01-06 12:16:12 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:16:12 INFO     hit@5:0.3604	hit@10:0.4681	ndcg@5:0.2522	ndcg@10:0.2872	
2023-01-06 12:16:38 INFO     ----------------------------Epoch 3----------------------------
2023-01-06 12:16:38 INFO      Training Time :[26.3 s]	Training Loss = 4.0299
2023-01-06 12:16:38 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:16:38 INFO     hit@5:0.4029	hit@10:0.5182	ndcg@5:0.2907	ndcg@10:0.328	
2023-01-06 12:17:04 INFO     ----------------------------Epoch 4----------------------------
2023-01-06 12:17:04 INFO      Training Time :[26.0 s]	Training Loss = 3.9667
2023-01-06 12:17:04 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:17:04 INFO     hit@5:0.4193	hit@10:0.5372	ndcg@5:0.3087	ndcg@10:0.3469	
2023-01-06 12:17:30 INFO     ----------------------------Epoch 5----------------------------
2023-01-06 12:17:30 INFO      Training Time :[25.9 s]	Training Loss = 3.9150
2023-01-06 12:17:30 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:17:30 INFO     hit@5:0.4466	hit@10:0.5593	ndcg@5:0.3343	ndcg@10:0.3708	
2023-01-06 12:17:56 INFO     ----------------------------Epoch 6----------------------------
2023-01-06 12:17:56 INFO      Training Time :[25.8 s]	Training Loss = 3.8625
2023-01-06 12:17:56 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:17:56 INFO     hit@5:0.4645	hit@10:0.5694	ndcg@5:0.3471	ndcg@10:0.3812	
2023-01-06 12:18:22 INFO     ----------------------------Epoch 7----------------------------
2023-01-06 12:18:22 INFO      Training Time :[25.8 s]	Training Loss = 3.8080
2023-01-06 12:18:22 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:18:22 INFO     hit@5:0.4849	hit@10:0.5851	ndcg@5:0.3676	ndcg@10:0.4003	
2023-01-06 12:18:47 INFO     ----------------------------Epoch 8----------------------------
2023-01-06 12:18:47 INFO      Training Time :[25.6 s]	Training Loss = 3.7622
2023-01-06 12:18:47 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:18:47 INFO     hit@5:0.4886	hit@10:0.5877	ndcg@5:0.3791	ndcg@10:0.4113	
2023-01-06 12:19:13 INFO     ----------------------------Epoch 9----------------------------
2023-01-06 12:19:13 INFO      Training Time :[26.0 s]	Training Loss = 3.7256
2023-01-06 12:19:13 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:19:13 INFO     hit@5:0.4937	hit@10:0.5916	ndcg@5:0.3802	ndcg@10:0.4119	
2023-01-06 12:19:39 INFO     ----------------------------Epoch 10----------------------------
2023-01-06 12:19:39 INFO      Training Time :[25.1 s]	Training Loss = 3.6925
2023-01-06 12:19:39 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:19:39 INFO     hit@5:0.4849	hit@10:0.5918	ndcg@5:0.3815	ndcg@10:0.4164	
2023-01-06 12:20:04 INFO     ----------------------------Epoch 11----------------------------
2023-01-06 12:20:04 INFO      Training Time :[25.2 s]	Training Loss = 3.6615
2023-01-06 12:20:04 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:20:04 INFO     hit@5:0.4921	hit@10:0.5883	ndcg@5:0.3819	ndcg@10:0.4129	
2023-01-06 12:20:30 INFO     ----------------------------Epoch 12----------------------------
2023-01-06 12:20:30 INFO      Training Time :[25.9 s]	Training Loss = 3.6286
2023-01-06 12:20:30 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:20:30 INFO     hit@5:0.4884	hit@10:0.5821	ndcg@5:0.3817	ndcg@10:0.4119	
2023-01-06 12:20:56 INFO     ----------------------------Epoch 13----------------------------
2023-01-06 12:20:56 INFO      Training Time :[25.6 s]	Training Loss = 3.6025
2023-01-06 12:20:56 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:20:56 INFO     hit@5:0.4844	hit@10:0.5785	ndcg@5:0.3853	ndcg@10:0.4158	
2023-01-06 12:21:22 INFO     ----------------------------Epoch 14----------------------------
2023-01-06 12:21:22 INFO      Training Time :[26.3 s]	Training Loss = 3.5723
2023-01-06 12:21:22 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:21:22 INFO     hit@5:0.4981	hit@10:0.593	ndcg@5:0.3916	ndcg@10:0.4221	
2023-01-06 12:21:48 INFO     ----------------------------Epoch 15----------------------------
2023-01-06 12:21:48 INFO      Training Time :[25.9 s]	Training Loss = 3.5452
2023-01-06 12:21:48 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:21:48 INFO     hit@5:0.488	hit@10:0.5834	ndcg@5:0.3868	ndcg@10:0.4176	
2023-01-06 12:22:14 INFO     ----------------------------Epoch 16----------------------------
2023-01-06 12:22:14 INFO      Training Time :[26.0 s]	Training Loss = 3.5171
2023-01-06 12:22:14 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:22:14 INFO     hit@5:0.4859	hit@10:0.5812	ndcg@5:0.3886	ndcg@10:0.4195	
2023-01-06 12:22:40 INFO     ----------------------------Epoch 17----------------------------
2023-01-06 12:22:40 INFO      Training Time :[26.1 s]	Training Loss = 3.4909
2023-01-06 12:22:40 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:22:40 INFO     hit@5:0.4902	hit@10:0.5823	ndcg@5:0.3895	ndcg@10:0.4191	
2023-01-06 12:23:07 INFO     ----------------------------Epoch 18----------------------------
2023-01-06 12:23:07 INFO      Training Time :[26.6 s]	Training Loss = 3.4621
2023-01-06 12:23:07 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:23:07 INFO     hit@5:0.4874	hit@10:0.5781	ndcg@5:0.3863	ndcg@10:0.4158	
2023-01-06 12:23:33 INFO     ----------------------------Epoch 19----------------------------
2023-01-06 12:23:33 INFO      Training Time :[26.4 s]	Training Loss = 3.4395
2023-01-06 12:23:33 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:23:33 INFO     hit@5:0.4953	hit@10:0.578	ndcg@5:0.3936	ndcg@10:0.4204	
2023-01-06 12:23:33 INFO     ------------------------Best Evaluation------------------------
2023-01-06 12:23:33 INFO     Best Result at Epoch: 14	 Early Stop at Patience: 5
2023-01-06 12:23:33 INFO     hit@5:0.4981	hit@10:0.593	ndcg@5:0.3916	ndcg@10:0.4221	
2023-01-06 12:23:34 INFO     -----------------------------Test Results------------------------------
2023-01-06 12:23:34 INFO     hit@5:0.4877	hit@10:0.5836	ndcg@5:0.3825	ndcg@10:0.4135	
