2023-01-06 12:25:06 INFO     log save at : log\grocery\MELOD_1.log
2023-01-06 12:25:06 INFO     model save at: save\MELOD-grocery-2023-01-06_12-25-06.pth
2023-01-06 12:25:06 INFO     [1] Model Hyper-Parameter ---------------------
2023-01-06 12:25:06 INFO     model: MELOD
2023-01-06 12:25:06 INFO     embed_size: 128
2023-01-06 12:25:06 INFO     DIIN_loss_type: MR
2023-01-06 12:25:06 INFO     alpha: 0.7
2023-01-06 12:25:06 INFO     lamda: 0.5
2023-01-06 12:25:06 INFO     sas_prob: 2
2023-01-06 12:25:06 INFO     freeze_kg: False
2023-01-06 12:25:06 INFO     episode_num: 1
2023-01-06 12:25:06 INFO     episode_len: 3
2023-01-06 12:25:06 INFO     hit_range: 100
2023-01-06 12:25:06 INFO     hit_r: 1.0
2023-01-06 12:25:06 INFO     sample_size: 100
2023-01-06 12:25:06 INFO     prob_sharpen: 1.0
2023-01-06 12:25:06 INFO     num_blocks: 2
2023-01-06 12:25:06 INFO     num_heads: 2
2023-01-06 12:25:06 INFO     ffn_hidden: 256
2023-01-06 12:25:06 INFO     attn_dropout: 0.5
2023-01-06 12:25:06 INFO     ffn_dropout: 0.5
2023-01-06 12:25:06 INFO     model_type: Knowledge
2023-01-06 12:25:06 INFO     loss_type: CUSTOM
2023-01-06 12:25:06 INFO     [2] Experiment Hyper-Parameter ----------------
2023-01-06 12:25:06 INFO     [2-1] data hyper-parameter --------------------
2023-01-06 12:25:06 INFO     dataset: grocery
2023-01-06 12:25:06 INFO     data_aug: True
2023-01-06 12:25:06 INFO     seq_filter_len: 3
2023-01-06 12:25:06 INFO     if_filter_target: True
2023-01-06 12:25:06 INFO     use_tar_len: True
2023-01-06 12:25:06 INFO     target_len: 3
2023-01-06 12:25:06 INFO     max_len: 50
2023-01-06 12:25:06 INFO     [2-2] pretraining hyper-parameter -------------
2023-01-06 12:25:06 INFO     do_pretraining: False
2023-01-06 12:25:06 INFO     pretraining_task: MISP
2023-01-06 12:25:06 INFO     pretraining_epoch: 10
2023-01-06 12:25:06 INFO     pretraining_batch: 512
2023-01-06 12:25:06 INFO     pretraining_lr: 0.001
2023-01-06 12:25:06 INFO     pretraining_l2: 0.0
2023-01-06 12:25:06 INFO     [2-3] training hyper-parameter ----------------
2023-01-06 12:25:06 INFO     epoch_num: 100
2023-01-06 12:25:06 INFO     train_batch: 512
2023-01-06 12:25:06 INFO     learning_rate: 0.001
2023-01-06 12:25:06 INFO     l2: 1e-06
2023-01-06 12:25:06 INFO     patience: 5
2023-01-06 12:25:06 INFO     device: cuda:0
2023-01-06 12:25:06 INFO     num_worker: 0
2023-01-06 12:25:06 INFO     [2-4] evaluation hyper-parameter --------------
2023-01-06 12:25:06 INFO     split_type: valid_and_test
2023-01-06 12:25:06 INFO     split_mode: LS_R@0.2
2023-01-06 12:25:06 INFO     eval_mode: uni100
2023-01-06 12:25:06 INFO     metric: ['hit', 'ndcg']
2023-01-06 12:25:06 INFO     k: [5, 10]
2023-01-06 12:25:06 INFO     valid_metric: hit@10
2023-01-06 12:25:06 INFO     eval_batch: 512
2023-01-06 12:25:06 INFO     [2-5] save hyper-parameter --------------------
2023-01-06 12:25:06 INFO     log_save: log
2023-01-06 12:25:06 INFO     model_save: save
2023-01-06 12:25:06 INFO     [3] Data Statistic ----------------------------
2023-01-06 12:25:06 INFO     dataset: grocery
2023-01-06 12:25:06 INFO     user number: 14681
2023-01-06 12:25:06 INFO     item number: 8714
2023-01-06 12:25:06 INFO     average seq length: 10.3027
2023-01-06 12:25:06 INFO     density: 0.0012 sparsity: 0.9988
2023-01-06 12:25:06 INFO     data after augmentation:
2023-01-06 12:25:06 INFO     train samples: 121892	eval samples: 2936	test samples: 11745
2023-01-06 12:25:06 INFO     [1] Model Architecture ------------------------
2023-01-06 12:25:06 INFO     total parameters: 1453186
2023-01-06 12:25:06 INFO     MELOD(
  (indu_loss_func): MarginRankingLoss()
  (item_embedding): Embedding(8715, 128)
  (seq_encoder): Transformer(
    (pos_emb): Embedding(50, 128)
    (emb_dropout): Dropout(p=0.5, inplace=False)
    (attention_layernorms): ModuleList(
      (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
    )
    (attention_layers): ModuleList(
      (0): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (1): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
    (forward_layernorms): ModuleList(
      (0): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
    )
    (forward_layers): ModuleList(
      (0): PointWiseFeedForward(
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (act): ReLU()
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (dropout2): Dropout(p=0.5, inplace=False)
      )
      (1): PointWiseFeedForward(
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (act): ReLU()
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (dropout2): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (W1): Linear(in_features=256, out_features=256, bias=True)
  (W2): Linear(in_features=256, out_features=2, bias=True)
  (emb_drop): Dropout(p=0.5, inplace=False)
  (cos): CosineSimilarity()
)
2023-01-06 12:25:06 INFO     Start training...
2023-01-06 12:25:28 INFO     ----------------------------Epoch 1----------------------------
2023-01-06 12:25:28 INFO      Training Time :[22.1 s]	Training Loss = 4.1506
2023-01-06 12:25:28 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:25:28 INFO     hit@5:0.3365	hit@10:0.4445	ndcg@5:0.2279	ndcg@10:0.2631	
2023-01-06 12:25:50 INFO     ----------------------------Epoch 2----------------------------
2023-01-06 12:25:50 INFO      Training Time :[21.5 s]	Training Loss = 3.8430
2023-01-06 12:25:50 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:25:50 INFO     hit@5:0.4009	hit@10:0.511	ndcg@5:0.2828	ndcg@10:0.3183	
2023-01-06 12:26:11 INFO     ----------------------------Epoch 3----------------------------
2023-01-06 12:26:11 INFO      Training Time :[21.2 s]	Training Loss = 3.7551
2023-01-06 12:26:11 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:26:11 INFO     hit@5:0.4107	hit@10:0.5291	ndcg@5:0.2922	ndcg@10:0.3305	
2023-01-06 12:26:33 INFO     ----------------------------Epoch 4----------------------------
2023-01-06 12:26:33 INFO      Training Time :[21.3 s]	Training Loss = 3.7010
2023-01-06 12:26:33 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:26:33 INFO     hit@5:0.4425	hit@10:0.5442	ndcg@5:0.3163	ndcg@10:0.3495	
2023-01-06 12:26:54 INFO     ----------------------------Epoch 5----------------------------
2023-01-06 12:26:54 INFO      Training Time :[21.6 s]	Training Loss = 3.6659
2023-01-06 12:26:54 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:26:54 INFO     hit@5:0.4385	hit@10:0.5529	ndcg@5:0.3209	ndcg@10:0.3579	
2023-01-06 12:27:15 INFO     ----------------------------Epoch 6----------------------------
2023-01-06 12:27:15 INFO      Training Time :[20.8 s]	Training Loss = 3.6390
2023-01-06 12:27:15 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:27:15 INFO     hit@5:0.4459	hit@10:0.5541	ndcg@5:0.3282	ndcg@10:0.3634	
2023-01-06 12:27:36 INFO     ----------------------------Epoch 7----------------------------
2023-01-06 12:27:36 INFO      Training Time :[21.2 s]	Training Loss = 3.6147
2023-01-06 12:27:36 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:27:36 INFO     hit@5:0.4571	hit@10:0.5572	ndcg@5:0.3378	ndcg@10:0.3703	
2023-01-06 12:27:57 INFO     ----------------------------Epoch 8----------------------------
2023-01-06 12:27:57 INFO      Training Time :[20.6 s]	Training Loss = 3.5927
2023-01-06 12:27:57 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:27:57 INFO     hit@5:0.454	hit@10:0.5552	ndcg@5:0.338	ndcg@10:0.3708	
2023-01-06 12:28:18 INFO     ----------------------------Epoch 9----------------------------
2023-01-06 12:28:18 INFO      Training Time :[20.5 s]	Training Loss = 3.5724
2023-01-06 12:28:18 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:28:18 INFO     hit@5:0.4581	hit@10:0.5636	ndcg@5:0.3474	ndcg@10:0.3816	
2023-01-06 12:28:38 INFO     ----------------------------Epoch 10----------------------------
2023-01-06 12:28:38 INFO      Training Time :[20.5 s]	Training Loss = 3.5570
2023-01-06 12:28:38 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:28:38 INFO     hit@5:0.4665	hit@10:0.571	ndcg@5:0.3504	ndcg@10:0.3839	
2023-01-06 12:28:59 INFO     ----------------------------Epoch 11----------------------------
2023-01-06 12:28:59 INFO      Training Time :[20.9 s]	Training Loss = 3.5410
2023-01-06 12:28:59 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:28:59 INFO     hit@5:0.4764	hit@10:0.5666	ndcg@5:0.3596	ndcg@10:0.3888	
2023-01-06 12:29:20 INFO     ----------------------------Epoch 12----------------------------
2023-01-06 12:29:20 INFO      Training Time :[20.6 s]	Training Loss = 3.5225
2023-01-06 12:29:20 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:29:20 INFO     hit@5:0.4731	hit@10:0.5726	ndcg@5:0.3594	ndcg@10:0.3917	
2023-01-06 12:29:40 INFO     ----------------------------Epoch 13----------------------------
2023-01-06 12:29:40 INFO      Training Time :[20.5 s]	Training Loss = 3.5099
2023-01-06 12:29:40 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:29:40 INFO     hit@5:0.4731	hit@10:0.5734	ndcg@5:0.3631	ndcg@10:0.3957	
2023-01-06 12:30:01 INFO     ----------------------------Epoch 14----------------------------
2023-01-06 12:30:01 INFO      Training Time :[20.5 s]	Training Loss = 3.4883
2023-01-06 12:30:01 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:30:01 INFO     hit@5:0.4834	hit@10:0.5781	ndcg@5:0.3725	ndcg@10:0.4035	
2023-01-06 12:30:22 INFO     ----------------------------Epoch 15----------------------------
2023-01-06 12:30:22 INFO      Training Time :[21.2 s]	Training Loss = 3.4709
2023-01-06 12:30:22 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:30:22 INFO     hit@5:0.4789	hit@10:0.5777	ndcg@5:0.3658	ndcg@10:0.3979	
2023-01-06 12:30:43 INFO     ----------------------------Epoch 16----------------------------
2023-01-06 12:30:43 INFO      Training Time :[21.2 s]	Training Loss = 3.4550
2023-01-06 12:30:43 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:30:43 INFO     hit@5:0.4846	hit@10:0.584	ndcg@5:0.3704	ndcg@10:0.4029	
2023-01-06 12:31:04 INFO     ----------------------------Epoch 17----------------------------
2023-01-06 12:31:04 INFO      Training Time :[21.0 s]	Training Loss = 3.4310
2023-01-06 12:31:05 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:31:05 INFO     hit@5:0.4886	hit@10:0.588	ndcg@5:0.3718	ndcg@10:0.4038	
2023-01-06 12:31:26 INFO     ----------------------------Epoch 18----------------------------
2023-01-06 12:31:26 INFO      Training Time :[21.2 s]	Training Loss = 3.4057
2023-01-06 12:31:26 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:31:26 INFO     hit@5:0.4847	hit@10:0.5891	ndcg@5:0.3769	ndcg@10:0.4107	
2023-01-06 12:31:47 INFO     ----------------------------Epoch 19----------------------------
2023-01-06 12:31:47 INFO      Training Time :[20.9 s]	Training Loss = 3.3868
2023-01-06 12:31:47 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:31:47 INFO     hit@5:0.4906	hit@10:0.584	ndcg@5:0.3707	ndcg@10:0.401	
2023-01-06 12:32:08 INFO     ----------------------------Epoch 20----------------------------
2023-01-06 12:32:08 INFO      Training Time :[21.1 s]	Training Loss = 3.3679
2023-01-06 12:32:08 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:32:08 INFO     hit@5:0.4864	hit@10:0.5905	ndcg@5:0.3734	ndcg@10:0.407	
2023-01-06 12:32:29 INFO     ----------------------------Epoch 21----------------------------
2023-01-06 12:32:29 INFO      Training Time :[20.6 s]	Training Loss = 3.3475
2023-01-06 12:32:29 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:32:29 INFO     hit@5:0.4852	hit@10:0.5875	ndcg@5:0.3761	ndcg@10:0.409	
2023-01-06 12:32:49 INFO     ----------------------------Epoch 22----------------------------
2023-01-06 12:32:49 INFO      Training Time :[20.5 s]	Training Loss = 3.3244
2023-01-06 12:32:49 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:32:49 INFO     hit@5:0.4894	hit@10:0.5863	ndcg@5:0.3792	ndcg@10:0.4103	
2023-01-06 12:33:10 INFO     ----------------------------Epoch 23----------------------------
2023-01-06 12:33:10 INFO      Training Time :[20.6 s]	Training Loss = 3.3079
2023-01-06 12:33:10 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:33:10 INFO     hit@5:0.4916	hit@10:0.5886	ndcg@5:0.3784	ndcg@10:0.4097	
2023-01-06 12:33:30 INFO     ----------------------------Epoch 24----------------------------
2023-01-06 12:33:30 INFO      Training Time :[20.5 s]	Training Loss = 3.2935
2023-01-06 12:33:30 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:33:30 INFO     hit@5:0.4878	hit@10:0.5909	ndcg@5:0.38	ndcg@10:0.413	
2023-01-06 12:33:51 INFO     ----------------------------Epoch 25----------------------------
2023-01-06 12:33:51 INFO      Training Time :[20.6 s]	Training Loss = 3.2689
2023-01-06 12:33:51 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:33:51 INFO     hit@5:0.4975	hit@10:0.5918	ndcg@5:0.3819	ndcg@10:0.4122	
2023-01-06 12:34:11 INFO     ----------------------------Epoch 26----------------------------
2023-01-06 12:34:11 INFO      Training Time :[20.5 s]	Training Loss = 3.2542
2023-01-06 12:34:11 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:34:11 INFO     hit@5:0.4763	hit@10:0.5793	ndcg@5:0.3704	ndcg@10:0.4039	
2023-01-06 12:34:32 INFO     ----------------------------Epoch 27----------------------------
2023-01-06 12:34:32 INFO      Training Time :[20.5 s]	Training Loss = 3.2378
2023-01-06 12:34:32 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:34:32 INFO     hit@5:0.4797	hit@10:0.5788	ndcg@5:0.3738	ndcg@10:0.4057	
2023-01-06 12:34:52 INFO     ----------------------------Epoch 28----------------------------
2023-01-06 12:34:52 INFO      Training Time :[20.5 s]	Training Loss = 3.2253
2023-01-06 12:34:52 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:34:52 INFO     hit@5:0.4938	hit@10:0.5868	ndcg@5:0.3844	ndcg@10:0.4142	
2023-01-06 12:35:13 INFO     ----------------------------Epoch 29----------------------------
2023-01-06 12:35:13 INFO      Training Time :[20.6 s]	Training Loss = 3.2107
2023-01-06 12:35:13 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:35:13 INFO     hit@5:0.4888	hit@10:0.5811	ndcg@5:0.3813	ndcg@10:0.4115	
2023-01-06 12:35:34 INFO     ----------------------------Epoch 30----------------------------
2023-01-06 12:35:34 INFO      Training Time :[20.5 s]	Training Loss = 3.1933
2023-01-06 12:35:34 INFO     Evaluation Time:[0.1 s]	  Eval Loss   = **
2023-01-06 12:35:34 INFO     hit@5:0.4843	hit@10:0.587	ndcg@5:0.3811	ndcg@10:0.4142	
2023-01-06 12:35:34 INFO     ------------------------Best Evaluation------------------------
2023-01-06 12:35:34 INFO     Best Result at Epoch: 25	 Early Stop at Patience: 5
2023-01-06 12:35:34 INFO     hit@5:0.4975	hit@10:0.5918	ndcg@5:0.3819	ndcg@10:0.4122	
2023-01-06 12:35:34 INFO     -----------------------------Test Results------------------------------
2023-01-06 12:35:34 INFO     hit@5:0.4984	hit@10:0.5947	ndcg@5:0.3898	ndcg@10:0.4209	
